<p align="center">
  <a href="https://github.com/tileshq/">
    <img src="https://avatars.githubusercontent.com/u/210493283?s=400&u=1ee6e44b6a683b16bdb6e9e853c7ebd8c7fd4268&v=4" alt="Tiles Logo" width="128" />
  </a>
</p>
<p align="center">
 Private, on-device AI memory that personalizes the agents you use, on your terms.
</p>
            <p>
 We'd love to partner with early-stage companies to build together. Join us in the Tiles <a href="https://discord.gg/yp3xQbHT" className="underline" target="_blank" rel="noopener noreferrer">Discord</a> server. Subscribe to our <a href="https://www.blog.tiles.run/" className="underline" target="_blank" rel="noopener noreferrer">blog</a> for updates on on-device AI and software personalization research. 

Consider supporting our work through <a href="https://github.com/sponsors/tileshq" className="underline" target="_blank" rel="noopener noreferrer">Github Sponsors</a>.

</p>

# Resources
Below is a living index of resources that inform and inspire our work.

## Motivations

- ✨ [This is Cuba's Netflix, Hulu, and Spotify – all without the internet](https://www.youtube.com/watch?v=fTTno8D-b2E)
- ✨ [The Memory Walled Garden](https://asimovaddendum.substack.com/p/the-memory-walled-garden)
- ✨ [Why Tech Needs Personalization](https://om.co/2025/10/29/why-tech-needs-personalization/)
- [Cuba's Underground Gaming Network](https://www.youtube.com/watch?v=lEplzHraw3c)

## Research

- ✨ [Evolution of Apple M Series Chips | From M1 to M5](https://www.youtube.com/watch?v=sLXzMK6JBV8)
- ✨ [The Continual Learning Problem, Jessy Lin](https://jessylin.com/2025/10/20/continual-learning/)
- ✨ [mem-agent: Equipping LLM Agents with Memory Using RL](https://dria.co/research/mem-agent:-equipping-llm-agents-with-memory-using-rl)
- ✨ [Jan-nano Technical Report](https://arxiv.org/abs/2506.22760)
- ✨ [LoRA Without Regret](https://thinkingmachines.ai/blog/lora/)
- ✨[A Preliminary Report On Edge-Verified Machine Learning (evML)](https://github.com/exo-explore/evML/blob/main/A_Preliminary_Report_On_evML.pdf)
- ✨ [Pretraining with hierarchical memories: separating long-tail and common knowledge, Apple](https://www.arxiv.org/abs/2510.02375)
- [LoRA Learns Less and Forgets Less](https://arxiv.org/abs/2405.09673)
- ✨ [The Bitter Lesson is coming for Tokenization](https://lucalp.dev/bitter-lesson-tokenization-and-blt/) 
- [On the Way to LLM Personalization: Learning to Remember User Conversations, Apple Machine Learning Research](https://machinelearning.apple.com/research/on-the-way)
- ✨ [Text-to-LoRA: Hypernetworks that adapt LLMs for specific benchmark tasks using only textual task description as the input ,Sakana AI](https://arxiv.org/abs/2506.06105)
- [Transformer²: Self-Adaptive LLMs](https://sakana.ai/transformer-squared/)
- [How memory augmentation can improve large language models, IBM Research](https://research.ibm.com/blog/memory-augmented-LLMs)
- [Mixture-of-Depths: Dynamically allocating compute in transformer-based language models](https://arxiv.org/abs/2404.02258)
- ✨ [The Power of Efficiency: Edge Al’s Role in Sustainable Generative Al Adoption](https://creativestrategies.com/research/gen-ai-edge-testing/)
- ✨ [Small Language Models are the Future of Agentic AI, NVIDIA Research](https://arxiv.org/abs/2506.02153)
- ✨ [Defeating Prompt Injections by Design, Google Deepmind](https://arxiv.org/abs/2503.18813)
- [LLM in a flash: Efficient Large Language Model Inference with Limited Memory](https://arxiv.org/abs/2312.11514)
- [Introducing FlexOlmo: a new paradigm for language model training and data collaboration, Allen AI](https://allenai.org/blog/flexolmo)
- [WhisperKit: On-device Real-time ASR with Billion-Scale Transformers, Argmax](https://openreview.net/attachment?id=6lC3MPFbVg&name=pdf)
- ✨ [Towards Large-scale Training on Apple Silicon, Exo Labs](https://openreview.net/pdf?id=TJjP8d5bms)
- [Kinetics: Rethinking Test-Time Scaling Laws](https://openreview.net/attachment?id=qxnJrm47Ag&name=pdf)
- [Enhancing Reasoning Capabilities of Small Language Models with Blueprints and Prompt Template Search](https://openreview.net/attachment?id=LsNstclw8Z&name=pdf)
- [LoFT: Low-Rank Adaptation That Behaves Like Full Fine-Tuning](https://arxiv.org/pdf/2505.21289)
- [AirLLM: Diffusion Policy-based Adaptive LoRA for Remote Fine-Tuning of LLM over the Air](https://arxiv.org/abs/2507.11515)
- [Comparative Analysis of Retrieval Systems in the Real World](https://arxiv.org/pdf/2405.02048)
- [FedVLM: Scalable Personalized Vision-Language Models through Federated Learning](https://arxiv.org/abs/2507.17088)
- [On the Way to LLM Personalization: Learning to Remember User Conversations](https://arxiv.org/abs/2411.13405)
- [A Preliminary Report On Edge-Verified Machine Learning, Exo Labs](https://github.com/exo-explore/evML/blob/main/A_Preliminary_Report_On_evML.pdf)
- ✨ [Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities](https://arxiv.org/abs/2408.07666)
- ✨ [Intent-Based Architecture and Their Risks](https://www.paradigm.xyz/2023/06/intents)
- [Your LLM Knows the Future: Uncovering Its Multi-Token Prediction Potential](https://arxiv.org/abs/2507.11851)
- [Cephalo: Multi-Modal Vision-Language Models for Bio-Inspired Materials Analysis and Design](https://arxiv.org/abs/2405.19076)
- [Towards Feasible Private Distributed LLM Inference, Dria](https://dria.co/research/towards-feasible-private-distributed-llm-inference)
- ✨ [ChatGPT Memory and the Bitter Lesson](https://www.shloked.com/writing/chatgpt-memory-bitter-lesson)
- ✨ [OpenPoke: Recreating Poke's Architecture](https://www.shloked.com/writing/openpoke)
- [Anthropic's Opinionated Memory Bet](https://shloked.com/writing/claude-memory-tool)
- [Mind the Trust Gap: Fast, Private Local-to-Cloud LLM Chat](https://hazyresearch.stanford.edu/blog/2025-05-12-security)

## Reference 
- ✨ [Foundation Models, Apple Developer Documentation](https://developer.apple.com/documentation/FoundationModels)
- ✨ [User-Agent header, MDN](https://developer.mozilla.org/en-US/docs/Web/HTTP/Reference/Headers/User-Agent)
- ✨ [Modeflile Reference, Ollama Documentation](https://ollama.readthedocs.io/en/modelfile/)
- [Dria Inference Arena – Compare Benchmarks Across LLMs, Inference Engines & Hardware](https://dria.co/inference-arena)
- [The GPU-Poor LLM Gladiator Arena](https://huggingface.co/spaces/k-mktr/gpu-poor-llm-arena)
- ✨ [The Kaitchup Index: A Leaderboard for Quantized LLMs](https://kaitchup.substack.com/p/the-kaitchup-index)
- [InferenceMAX™: Open Source Inference Benchmarking](https://newsletter.semianalysis.com/p/inferencemax-open-source-inference)
- [RFT, DPO, SFT: Fine-tuning with OpenAI — Ilan Bigio, OpenAI](https://www.youtube.com/watch?v=JfaLQqfXqPA)
- ✨ [Hand-picked selection of articles on AI fundamentals/concepts that cover the entire process of building neural nets to training them to evaluating results.](https://aman.ai/primers/ai/)
- ✨ [The State of Open Models](https://youtu.be/FUcilE5Gx_0?si=9aP_NAzTKi8qw78n)
- ✨ [The State of On-Device LLMs](https://app.getcontrast.io/register/sota-the-state-of-llms)
- [How to Scale Your Model](https://jax-ml.github.io/scaling-book/)
- ✨ [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/)
- ✨ [An Analogy for Understanding Transformers](https://www.lesswrong.com/posts/euam65XjigaCJQkcN/an-analogy-for-understanding-transformers)
- ✨ [Neural networks, 3Blue1Brown](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)
- [GGUF Quantization Docs (Unofficial)](https://github.com/iuliaturc/gguf-docs)
- [Reverse-engineering GGUF | Post-Training Quantization](https://www.youtube.com/watch?v=vW30o4U9BFE)
- [Choosing a GGUF Model: K-Quants, I-Quants, and Legacy Formats](https://kaitchup.substack.com/p/choosing-a-gguf-model-k-quants-i)
- [Reference implementation of the Transformer architecture optimized for Apple Neural Engine](https://github.com/apple/ml-ane-transformers)
- [LLMs on a Budget](https://benjaminmarie.gumroad.com/l/llms-on-a-budget)

-------
Copyright © 2025, Tiles Privacy. All rights reserved.
