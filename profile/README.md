<p align="center">
  <a href="https://github.com/tileshq/">
    <img src="https://avatars.githubusercontent.com/u/210493283?s=400&u=1ee6e44b6a683b16bdb6e9e853c7ebd8c7fd4268&v=4" alt="Tiles Logo" width="128" />
  </a>
</p>
<p align="center">
  Decentralized memory custodian for personalized agentic experiences
</p>
            <p>
 We'd love to partner with early-stage companies to build together. Join us in the Tiles <a href="https://discord.gg/yp3xQbHT" className="underline" target="_blank" rel="noopener noreferrer">Discord</a> server. Subscribe to our blog <a href="https://neurons.pub/" className="underline" target="_blank" rel="noopener noreferrer">Neurons</a> for updates on on-device AI and personalization research. 

Consider supporting our research and open-source projects through <a href="https://github.com/sponsors/tileshq" className="underline" target="_blank" rel="noopener noreferrer">Github Sponsors</a>.

</p>

# Resources
Below is a living index of resources that inform and inspire our work.

## Engineering

- ‚ú® [Modelfile Reference - Ollama English Documentation](https://ollama.readthedocs.io/en/modelfile/)
- ‚ú® [Introducing Gemma 3n: The developer guide](https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/)
- ‚ú® [Foundation Models adapter training - Apple Intelligence - Apple Developer](https://developer.apple.com/apple-intelligence/foundation-models-adapter/)
- [vLLM Semantic Router: Next Phase in LLM inference](https://blog.vllm.ai/2025/09/11/semantic-router.html)
- ‚ú® [Use MergeKit to Extract LoRA Adapters from any Fine-Tuned Model](https://www.arcee.ai/blog/use-mergekit-to-extract-lora-adapters-from-any-fine-tuned-model)
- ‚ú® [Apple‚Äôs New Containerization Framework: A Deep Dive into macOS‚Äôs Future for Developers](https://chamodshehanka.medium.com/apples-new-containerization-framework-a-deep-dive-into-macos-s-future-for-developers-cf102643394a)
- [instavm/coderunner: A secure local sandbox to run LLM-generated code using Apple containers](https://github.com/instavm/coderunner)
- [Introducing the unified multi-modal MLX engine architecture in LM Studio](https://lmstudio.ai/blog/unified-mlx-engine)
- [Announcing Spiral, Data 3.0, with backing from the best](https://spiraldb.com/post/announcing-spiral)
- [mem-agent: Persistent, Human Readable Memory Agent Trained with Online RL](https://huggingface.co/blog/driaforall/mem-agent)
- ‚ú® [Optimizing AI Inference at Character.AI](https://research.character.ai/optimizing-inference/)
- ‚ú® [Optimizing AI Inference at Character.AI (Part Deux)](https://research.character.ai/optimizing-ai-inference-at-character-ai-part-deux/)
- ‚ú® [PrimeIntellect-ai/prime-iroh: Asynchronous P2P communication backend for decentralized pipeline parallelism](https://github.com/PrimeIntellect-ai/prime-iroh)
- ‚ú® [Introducing Gemma 3 270M: The compact model for hyper-efficient AI](https://developers.googleblog.com/en/introducing-gemma-3-270m/)
- ‚ú® [Unternet Kernel](https://github.com/unternet-co/client/tree/main/kernel)
- ‚ú® [SwiftWasm, WebAssembly support for the Swift programming language](https://github.com/swiftwasm/swift)
- [DSPy Notebook, The pretty much "official" DSPy framework for Typescript](https://github.com/ax-llm/ax)
- [Structured outputs for LLMs](https://github.com/dottxt-ai/outlines)
- [Accelerated PyTorch training on Mac](https://docs.pytorch.org/docs/stable/notes/mps.html)
- ‚ú® [Unsloth AI - Open Source Fine-tuning & RL for LLMs](https://unsloth.ai/)
- ‚ú® [Introducing LFM2: The Fastest On-Device Foundation Models on the Market](https://www.liquid.ai/blog/liquid-foundation-models-v2-our-second-series-of-generative-ai-models)
- ‚ú® [Mistral.rs, a cross-platform, highly-multimodal inference engine](https://github.com/EricLBuehler/mistral.rs)
- [Osmosis, Unlocking AI self-improvement at production scale](https://osmosis.ai/)
- [Supermemory MCP](https://mcp.supermemory.ai/)
- ‚ú® [Introducing the v0 composite model family, Vercel](https://vercel.com/blog/v0-composite-model-family#why-does-v0-need-a-composite-model-architecture?)
- [Agent Reinforcement Trainer, OpenPipe](https://github.com/openpipe/art)
- [Universal Quantized File Format: UQFF](https://github.com/EricLBuehler/mistral.rs/blob/master/docs/UQFF.md)
- [GGUF Tool Suite](https://github.com/Thireus/GGUF-Tool-Suite/)
- [uqff_maker](https://github.com/EricLBuehler/uqff_maker)
- [Minions, Big & Small LLMs working together](https://github.com/HazyResearch/minions)
- ‚ú® [The Kaitchup Index: A Leaderboard for Quantized LLMs](https://kaitchup.substack.com/p/the-kaitchup-index)
- [Pipecat Cloud: Enterprise Voice Agents Built On Open Source - Kwindla Hultman Kramer, Daily](https://www.youtube.com/watch?v=IA4lZjh9sTs)
- [Serving Voice AI at $1/hr: Open-source, LoRAs, Latency, Load Balancing - Neil Dwyer, Gabber](https://www.youtube.com/watch?v=rD23-VZZHOo)
- [üìèRULER: Easy Mode for RL Rewards](https://openpipe.ai/blog/ruler)
- [ART¬∑E: How We Built an Email Research Agent That Beats o3](https://openpipe.ai/blog/art-e-mail-agent)
- [OpenBench, Provider-agnostic, open-source evaluation infrastructure for language models](https://github.com/groq/openbench)
- ‚ú® [LoRA's Limitations: Head-to-Head with Full RL](https://osmosis.ai/blog/lora-comparison)
- ‚ú® [A case for client-side machine learning, Christopher Fleetwood](https://web.archive.org/web/20250512221340/https://fleetwood.dev/posts/a-case-for-client-side-machine-learning)
- [Democratizing Al: The Psyche Network Architecture, Nous Research](https://nousresearch.com/nous-psyche/)
- [Interoperability: Swift‚Äôs Super Power, Speaking in Swift by The Browser Company](https://speakinginswift.substack.com/p/interoperability-swifts-super-power)


## Research

- ‚ú® [LoRA Without Regret](https://thinkingmachines.ai/blog/lora/)
- ‚ú® [Pretraining with hierarchical memories: separating long-tail and common knowledge, Apple](https://www.arxiv.org/abs/2510.02375)
- [LoRA Learns Less and Forgets Less](https://arxiv.org/abs/2405.09673)
- ‚ú® [The Bitter Lesson is coming for Tokenization](https://lucalp.dev/bitter-lesson-tokenization-and-blt/) 
- [On the Way to LLM Personalization: Learning to Remember User Conversations, Apple Machine Learning Research](https://machinelearning.apple.com/research/on-the-way)
- ‚ú® [Text-to-LoRA: Hypernetworks that adapt LLMs for specific benchmark tasks using only textual task description as the input ,Sakana AI](https://arxiv.org/abs/2506.06105)
- [Transformer¬≤: Self-Adaptive LLMs](https://sakana.ai/transformer-squared/)
- [How memory augmentation can improve large language models, IBM Research](https://research.ibm.com/blog/memory-augmented-LLMs)
- [Mixture-of-Depths: Dynamically allocating compute in transformer-based language models](https://arxiv.org/abs/2404.02258)
- ‚ú® [The Power of Efficiency: Edge Al‚Äôs Role in Sustainable Generative Al Adoption](https://creativestrategies.com/research/gen-ai-edge-testing/)
- ‚ú® [Small Language Models are the Future of Agentic AI, NVIDIA Research](https://arxiv.org/abs/2506.02153)
- ‚ú® [Defeating Prompt Injections by Design, Google Deepmind](https://arxiv.org/abs/2503.18813)
- [LLM in a flash: Efficient Large Language Model Inference with Limited Memory](https://arxiv.org/abs/2312.11514)
- [Introducing FlexOlmo: a new paradigm for language model training and data collaboration, Allen AI](https://allenai.org/blog/flexolmo)
- [WhisperKit: On-device Real-time ASR with Billion-Scale Transformers, Argmax](https://openreview.net/attachment?id=6lC3MPFbVg&name=pdf)
- ‚ú® [Towards Large-scale Training on Apple Silicon, Exo Labs](https://openreview.net/pdf?id=TJjP8d5bms)
- [Kinetics: Rethinking Test-Time Scaling Laws](https://openreview.net/attachment?id=qxnJrm47Ag&name=pdf)
- [Enhancing Reasoning Capabilities of Small Language Models with Blueprints and Prompt Template Search](https://openreview.net/attachment?id=LsNstclw8Z&name=pdf)
- [LoFT: Low-Rank Adaptation That Behaves Like Full Fine-Tuning](https://arxiv.org/pdf/2505.21289)
- [AirLLM: Diffusion Policy-based Adaptive LoRA for Remote Fine-Tuning of LLM over the Air](https://arxiv.org/abs/2507.11515)
- [Comparative Analysis of Retrieval Systems in the Real World](https://arxiv.org/pdf/2405.02048)
- [FedVLM: Scalable Personalized Vision-Language Models through Federated Learning](https://arxiv.org/abs/2507.17088)
- [On the Way to LLM Personalization: Learning to Remember User Conversations](https://arxiv.org/abs/2411.13405)
- [A Preliminary Report On Edge-Verified Machine Learning, Exo Labs](https://github.com/exo-explore/evML/blob/main/A_Preliminary_Report_On_evML.pdf)
- ‚ú® [Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities](https://arxiv.org/abs/2408.07666)
- ‚ú® [Intent-Based Architecture and Their Risks](https://www.paradigm.xyz/2023/06/intents)
- [Your LLM Knows the Future: Uncovering Its Multi-Token Prediction Potential](https://arxiv.org/abs/2507.11851)
- [Cephalo: Multi-Modal Vision-Language Models for Bio-Inspired Materials Analysis and Design](https://arxiv.org/abs/2405.19076)
- [Towards Feasible Private Distributed LLM Inference, Dria](https://dria.co/research/towards-feasible-private-distributed-llm-inference)


## Reference 
- [RFT, DPO, SFT: Fine-tuning with OpenAI ‚Äî Ilan Bigio, OpenAI](https://www.youtube.com/watch?v=JfaLQqfXqPA)
- ‚ú® [Hand-picked selection of articles on AI fundamentals/concepts that cover the entire process of building neural nets to training them to evaluating results.](https://aman.ai/primers/ai/)
- ‚ú® [The State of On-Device LLMs](https://app.getcontrast.io/register/sota-the-state-of-llms)
- ‚ú® [Planetary-Scale Inference: Previewing our Peer-To-Peer Decentralized Inference Stack](https://www.primeintellect.ai/blog/inference)
- [How to Scale Your Model](https://jax-ml.github.io/scaling-book/)
- ‚ú® [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/)
- ‚ú® [An Analogy for Understanding Transformers](https://www.lesswrong.com/posts/euam65XjigaCJQkcN/an-analogy-for-understanding-transformers)
- ‚ú® [Neural networks, 3Blue1Brown](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)
- [GGUF Quantization Docs (Unofficial)](https://github.com/iuliaturc/gguf-docs)
- [Reverse-engineering GGUF | Post-Training Quantization](https://www.youtube.com/watch?v=vW30o4U9BFE)
- [Reference implementation of the Transformer architecture optimized for Apple Neural Engine](https://github.com/apple/ml-ane-transformers)
- [H100 PCIe vs SXM vs NVL: Which H100 GPU Is Fastest and Most Cost-Effective for Fine-Tuning LLMs?](https://kaitchup.substack.com/p/h100-pcie-vs-sxm-vs-nvl-best-single)
- [Apple Developer, Technotes, Learn about specific development topics through these in-depth technical articles.](https://developer.apple.com/documentation/technotes/)
- [The Apple Wiki](https://theapplewiki.com/wiki/Main_Page)
- [LLMs on a Budget](https://benjaminmarie.gumroad.com/l/llms-on-a-budget)

Resource inspired from [GPU Glossary, Modal](https://modal.com/gpu-glossary/perf)

-------
Copyright ¬© 2025, Tiles. All rights reserved.
